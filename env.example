# Local Model Configuration (Ollama)
LOCAL_API_BASE=http://localhost:11434
LOCAL_MODEL_NAME=llama3.2
USE_OLLAMA=true

# Alternative: OpenAI-compatible API endpoint
# LOCAL_API_BASE=http://localhost:1234/v1
# LOCAL_MODEL_NAME=your-model-name
# USE_OLLAMA=false

# Server Configuration
PORT=8000

